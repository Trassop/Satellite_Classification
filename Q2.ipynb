{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO8KONkGD/WI85kY8dNYx3s"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Similarly to the first notebook, I am importing relevant modules"
      ],
      "metadata": {
        "id": "nDqp9gNBuyOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xKPNDCkaGQGy"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make sure that the test/train split is the same as the first notebook the random seed is fixed, this is to test the accuracy in a way that's reproducable."
      ],
      "metadata": {
        "id": "tD_l_uGEGyZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "fRDf97a7GR_A"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing and loading the data is identical to Notebook 1."
      ],
      "metadata": {
        "id": "8OienLpfGzCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "satellite_data = pd.read_csv('Satellite_Data.csv')\n",
        "columns_of_interest = [\"Class of Orbit\", \"Type of Orbit\", \"Perigee (Kilometers)\",\n",
        "                       \"Apogee (Kilometers)\", \"Inclination (Degrees)\"]\n",
        "processed_data = satellite_data[columns_of_interest].copy()\n",
        "\n",
        "processed_data[\"Class of Orbit\"] = processed_data[\"Class of Orbit\"].fillna(\"Unknown\")\n",
        "processed_data[\"Type of Orbit\"] = processed_data[\"Type of Orbit\"].fillna(\"Unknown\")\n",
        "processed_data[\"Perigee (Kilometers)\"] = processed_data[\"Perigee (Kilometers)\"].fillna(\n",
        "    processed_data[\"Perigee (Kilometers)\"].mean())\n",
        "processed_data[\"Apogee (Kilometers)\"] = processed_data[\"Apogee (Kilometers)\"].fillna(\n",
        "    processed_data[\"Apogee (Kilometers)\"].mean())\n",
        "processed_data[\"Inclination (Degrees)\"] = processed_data[\"Inclination (Degrees)\"].fillna(0)\n",
        "\n",
        "processed_data[\"Altitude (Kilometers)\"] = processed_data.apply(\n",
        "    lambda row: row[\"Perigee (Kilometers)\"] if row[\"Apogee (Kilometers)\"] == 0 else\n",
        "                row[\"Apogee (Kilometers)\"] if row[\"Perigee (Kilometers)\"] == 0 else\n",
        "                (row[\"Perigee (Kilometers)\"] + row[\"Apogee (Kilometers)\"]) / 2, axis=1)"
      ],
      "metadata": {
        "id": "XIg5hCVUGUTh"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again features and targets is done similarly to Notebook 1, however LabelEncoder is used to convert the categorical data to numerical values for the neural network to interpret."
      ],
      "metadata": {
        "id": "Ol0ncY47Gztj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features and targets\n",
        "X = processed_data[[\"Altitude (Kilometers)\", \"Inclination (Degrees)\"]]\n",
        "y_class = LabelEncoder().fit_transform(processed_data[\"Class of Orbit\"])\n",
        "y_type = LabelEncoder().fit_transform(processed_data[\"Type of Orbit\"])"
      ],
      "metadata": {
        "id": "w-5Vk0zmGWlt"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the neural network a validation set is also used, some neural networks use stratified datasets - however the large number of classes combined with the low dataset size means that this is impractical."
      ],
      "metadata": {
        "id": "gLEgrSYKG0HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine `y_class` and `y_type` into a single 1D array\n",
        "combined_targets = y_class * len(np.unique(y_type)) + y_type\n",
        "\n",
        "# Split into training/validation/test sets\n",
        "X_train, X_test, y_train_combined, y_test_combined = train_test_split(\n",
        "    X, combined_targets, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Pn8hW_2mGZfW"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The combined array data for analysis is split back into class/type."
      ],
      "metadata": {
        "id": "N589LlLHG0ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the combined targets back into `y_class` and `y_type`\n",
        "y_train_class = y_train_combined // len(np.unique(y_type))\n",
        "y_train_type = y_train_combined % len(np.unique(y_type))\n",
        "y_test_class = y_test_combined // len(np.unique(y_type))\n",
        "y_test_type = y_test_combined % len(np.unique(y_type))\n"
      ],
      "metadata": {
        "id": "Tme3V7YzGbOa"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual split of the training/validation sets."
      ],
      "metadata": {
        "id": "ZlEdDu7UG01u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Further split training data into training and validation sets\n",
        "X_train, X_val, y_train_class, y_val_class, y_train_type, y_val_type = train_test_split(\n",
        "    X_train, y_train_class, y_train_type, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "JOvP6Y5PGcJs"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardising the features using StandardScaler normalised the data to a mean of 0 and a standard deviation of 1. This is to help optimise the NN."
      ],
      "metadata": {
        "id": "oGREN7RFG1NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "kKiSeo_2GdJd"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks require tensors to read, which the below code is converting the data to using the Torch module."
      ],
      "metadata": {
        "id": "z1aKPNAPG1ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_class_tensor = torch.tensor(y_train_class, dtype=torch.long)\n",
        "y_train_type_tensor = torch.tensor(y_train_type, dtype=torch.long)\n",
        "\n",
        "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
        "y_val_class_tensor = torch.tensor(y_val_class, dtype=torch.long)\n",
        "y_val_type_tensor = torch.tensor(y_val_type, dtype=torch.long)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_class_tensor = torch.tensor(y_test_class, dtype=torch.long)\n",
        "y_test_type_tensor = torch.tensor(y_test_type, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "7usHbGwTGeIX"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Neural Network is defined here. It is a Multi Output NN, as it defines both Class and Type of Orbit. Within the initialisation function the variables to be defined are the processed data with input_dim, and the outputs of Class and Type. Within this function the specific layering for the neural network is defined with fully connected as fc and batch normalisation bn. Batch normalisation speeds up training. This is then done a second time to refine the features of fc1. self.dropout prevents overfitting and self.relu helps to identify complexity. the forward function defines how data flows through the neural network."
      ],
      "metadata": {
        "id": "5hs8k0jNG1vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiOutputNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, class_output_dim, type_output_dim):\n",
        "        super(MultiOutputNeuralNetwork, self).__init__()\n",
        "        self.shared_fc1 = nn.Linear(input_dim, 64)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.shared_fc2 = nn.Linear(64, 32)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)  # Dropout for regularization\n",
        "        self.class_output = nn.Linear(32, class_output_dim)\n",
        "        self.type_output = nn.Linear(32, type_output_dim)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.shared_fc1(x)))\n",
        "        x = self.dropout(self.relu(self.bn2(self.shared_fc2(x))))\n",
        "        class_out = self.softmax(self.class_output(x))\n",
        "        type_out = self.softmax(self.type_output(x))\n",
        "        return class_out, type_out\n"
      ],
      "metadata": {
        "id": "CU1p6e5eGf_O"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model then has to be trained, and evaluated. First the model is instantiated, additional support code is defined, then it is trained."
      ],
      "metadata": {
        "id": "NnXxJ_ltG2cK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = MultiOutputNeuralNetwork(\n",
        "    input_dim=X_train_tensor.shape[1],\n",
        "    class_output_dim=len(np.unique(y_class)),\n",
        "    type_output_dim=len(np.unique(y_type))\n",
        ")"
      ],
      "metadata": {
        "id": "pIXTuJa5Gj_p"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A loss function is a measurement on how well the model performs, an optimiser updates the model's parameters during training, and the scheduler adjusts the learning rate to improve efficiency."
      ],
      "metadata": {
        "id": "ixMx-OnkG2xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer, scheduler, and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "NFxJkFRZGlx6"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train loop has to iterate enough times to be accurate, but not so much that the time taken is exponentially longer than the gain in accuracy. In this code it runs for 100 epochs, or iterations."
      ],
      "metadata": {
        "id": "CJKzOxX1G3HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train_model(model, optimizer, scheduler, criterion, epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        # Training step\n",
        "        optimizer.zero_grad()\n",
        "        class_out, type_out = model(X_train_tensor)\n",
        "        loss_class = criterion(class_out, y_train_class_tensor)\n",
        "        loss_type = criterion(type_out, y_train_type_tensor)\n",
        "        loss = loss_class + loss_type\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_class_out, val_type_out = model(X_val_tensor)\n",
        "            val_loss_class = criterion(val_class_out, y_val_class_tensor)\n",
        "            val_loss_type = criterion(val_type_out, y_val_type_tensor)\n",
        "            val_loss = val_loss_class + val_loss_type\n",
        "        model.train()\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Print progress\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "QAUlYOj_Gno_"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop is run."
      ],
      "metadata": {
        "id": "LMzZUEs_G3hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train_model(model, optimizer, scheduler, criterion, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuF9QInkGq1r",
        "outputId": "619764ae-7564-46c9-f766-d2ebb2506228"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train Loss: 3.9350, Val Loss: 3.9553\n",
            "Epoch 11/50, Train Loss: 3.7411, Val Loss: 3.8658\n",
            "Epoch 21/50, Train Loss: 3.5949, Val Loss: 3.6793\n",
            "Epoch 31/50, Train Loss: 3.4468, Val Loss: 3.4758\n",
            "Epoch 41/50, Train Loss: 3.3093, Val Loss: 3.3086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to notebook 1, the predicted data is compared to the actual data."
      ],
      "metadata": {
        "id": "pb7-kYRNG4Bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_class, y_pred_type = model(X_test_tensor)\n",
        "    y_pred_class = y_pred_class.argmax(dim=1).numpy()\n",
        "    y_pred_type = y_pred_type.argmax(dim=1).numpy()"
      ],
      "metadata": {
        "id": "b7ZXLfiGz2nM"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined accuracy\n",
        "combined_correct = (y_pred_class == y_test_class_tensor.numpy()) & (y_pred_type == y_test_type_tensor.numpy())\n",
        "combined_accuracy = combined_correct.mean() * 100\n",
        "print(f\"Combined Accuracy: {combined_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxXPrxvx1WlW",
        "outputId": "ae071242-7a39-444d-d514-d5396ce11c02"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Accuracy: 84.86%\n"
          ]
        }
      ]
    }
  ]
}